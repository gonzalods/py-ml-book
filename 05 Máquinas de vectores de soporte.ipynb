{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificación de margen máximo con máquinas de vectores de soporte#\n",
    "Otro algoritmo de aprendizaje potente y ampliamente utilizado es la **máquina de vectores de soporte (SVM)**, que puede considerarse una extensión del perceptrón. Al utilizar el algoritmo perceptron, se minimizan los errores de clasificación errónea. Sin embargo, en SVM el objetivo de optimización es maximizar el margen. El **margen** se define como la distancia entre el hiperplano de separación (límite de decisión) y los ejemplos de entrenamiento más cercanos a este hiperplano, que son los llamados **vectores de soporte**.\n",
    "\n",
    "![Margenes maximizados](imgs/maxmargin.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intuición sobre el margen máximo ###\n",
    "La razón detras de tener límites de decisión con márgenes grandes es que tienden a tener menores errores de generalización, mientras que los modelos con márgenes pequeños son más propensos al sobreajuste. Para tener una idea sobre la maximización de margenes, nos vamos a centrar esos hiperplanos positivo y negativo paralelos a el límite de decisión, que se pueden expresar como:\n",
    "\n",
    "$$\\begin{equation*} w_0 + \\mathbf{w}^T\\mathbf{x}_{pos}=1\\end{equation*}$$\n",
    "$$\\begin{equation*} w_0 + \\mathbf{w}^T\\mathbf{x}_{neg}=-1\\end{equation*}$$\n",
    "\n",
    "Si restamos estas dos ecuaciones, se tiene:\n",
    "\n",
    "$$\\Rightarrow \\mathbf{w}^T \\big(\\mathbf{x}_{pos} - \\mathbf{x}_{new} \\big) = 2 $$\n",
    "\n",
    "Se pude normalizar la ecuación mediante la norma o longitud del vector $w$, que se define como:\n",
    "\n",
    "$$ \\begin{Vmatrix}\\mathbf{w}\\end{Vmatrix} = \\sqrt{\\sum_{j=1}^m w_j^2}$$\n",
    "\n",
    "Así se llega a la siguiente equación:\n",
    "\n",
    "$$\\begin{equation*}\\frac{ \\mathbf{w}^T \\big(\\mathbf{x}_{pos} - \\mathbf{x}_{neg} \\big)}{\\begin{Vmatrix}\\mathbf{w}\\end{Vmatrix}} = \\frac{2}{\\begin{Vmatrix}\\mathbf{w}\\end{Vmatrix}} \\end{equation*}$$\n",
    "\n",
    "La exprsión de la izquierda se puede interpretar como la distancia entre el hiperploano positivo y negativo, que es lo que se denomina **margen** y se quiere maximizar.\n",
    "\n",
    "Ahora, la función objetivo de la SVM se convierte en la maximización de este margen maximizando $\\frac{2}{\\begin{Vmatrix}\\mathbf{w}\\end{Vmatrix}}$ bajo la restrición de que los ejemplos se han clasificado correctamente, que se puede escribir como:\n",
    "\n",
    "$$\\begin{equation*}w_0 + \\mathbf{w}^T\\mathbf{x}^{(i)} \\geq 1\\,\\, si y^{(i)}=1 \\end{equation*}$$\n",
    "\n",
    "$$\\begin{equation*}w_0 + \\mathbf{w}^T\\mathbf{x}^{(i)} \\leq -1\\,\\, si y^{(i)}=-1\\end{equation*}$$\n",
    "\n",
    "$$ para\\,i = 1...N$$\n",
    "\n",
    "Donde $N$ es el número de ejemplos en el conjunto de datos.\n",
    "\n",
    "Estas dos ecuaciones básicamente dicen que todos los ejemplos de clase negativa deben caer en el lado negativo del hiperplano negativo, mientras que los ejemplos de clase positiva caeran en el lado positivo del hiperplano positivo, lo cual se puede escribri de forma más compacta como sigue:\n",
    "\n",
    "$$\\begin{equation}y^{(i)}\\big(w_0 + \\mathbf{w}^T\\mathbf{x}^{(i)}\\big) \\geq 1\\,\\, \\forall i\\end{equation}$$\n",
    "\n",
    "En la práctica es más sencillo minimizar el termino reciproco, $\\frac{1}{2}\\begin{Vmatrix}\\mathbf{w}\\end{Vmatrix}^2$, que se puede resolver mediante programación cuadrática"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
